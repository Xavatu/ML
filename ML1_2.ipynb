{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igdr_UfSXCpE"
   },
   "source": [
    "# **Занятие 2. Модели PyTorch**\n",
    "\n",
    "https://vk.com/lambda_brain\n",
    "\n",
    "Модели в машинном обучении реализуют концепцию прогноза, предсказания, в виде единой целостной сущности (класс, объект). Они же в прикладном плане служат основной для создания всех видов нейронных сетей.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYVVNj5fqIa2"
   },
   "source": [
    "##**Подготовка данных**\n",
    "\n",
    "Для корректного продолжения работы в текущем ноутбуке требуется выполнить ряд инициализаций из предыдущего задания. Запустите следующий код, формирующий тестовые наборы даннх:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "bqXjx3EBsWLO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "np.random.seed(42)\n",
    "sz = 100\n",
    "x = np.random.rand(sz, 1)\n",
    "y = 1 + 2 * x + 0.1 * np.random.randn(sz, 1)\n",
    "idx = np.arange(sz)\n",
    "np.random.shuffle(idx)\n",
    "sz80 = (int)(sz*0.8)\n",
    "train_idx = idx[: sz80]\n",
    "val_idx = idx[sz80:]\n",
    "x_train, y_train = x[train_idx], y[train_idx]\n",
    "x_val, y_val = x[val_idx], y[val_idx]\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train).float().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyjZGmRVXlr0"
   },
   "source": [
    "Модель PyTorch представляет собой обычный класс Python, унаследованный от класса **nn.Module**.\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html?source=post_page---------------------------#torch.nn.Module\n",
    "\n",
    "Самых важных методов, которые потребуется переопределить в нашей модели, два:\n",
    "\n",
    "1) **стандартный конструктор**\n",
    "\n",
    "В конструкторе задаются базовые атрибуты, в нашем случае это два параметра a и b. Они определяются в конструкторе с помощью специального типа nn.Parameter.\n",
    "\n",
    "Количество атрибутов модели не ограничено.\n",
    "\n",
    "2) **forward()**, непосредственно выполняющий все нужные вычисления на основании входных данных (они задаются как параметр этого метода).\n",
    "\n",
    "Однако даже сами эти методы не потребуется вызывать напрямую. **В PyTorch \"запускается\" сама модель в целом**, и создание нужного объекта и вызов forward() происходят автоматически -- они так же скрыты под капотом.\n",
    "\n",
    "Вот как будет выглядеть наша модель для линейной регрессии:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2sg4zKSt7gbs"
   },
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "\n",
    "class ManualLinearRegression(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # два наших параметра a и b\n",
    "        self.a = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "        self.b = nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # формула линейной регрессии\n",
    "        return self.a + self.b * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bbMVxmZyYdtU"
   },
   "source": [
    "Тип nn.Parameter очень удобен тем, что мы можем автоматизировать многие моменты, связанные с обработкой параметров. Например метод parameters() позволяет организовать итерацию по всем параметрам модели, и даже по параметрам вложенных моделей, которые могут потребоваться для настройки оптимизатора (вместо того, чтобы формировать такой список параметров вручную).\n",
    "\n",
    "Текущие значения всех параметров можно получить с помощью метода state_dict().\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJ4Lu_lrY1nO"
   },
   "source": [
    "**Важно.** Все наши выборки надо располагать на том же девайсе, где мы размещаем и модель. Это необходимо из соображений эффективности. Напомню, что данные основных типов PyTorch загружаются на девайс методом to().\n",
    "\n",
    "\n",
    "\n",
    "Наконец, нам ещё потребуется вызвать вручную метод модели **train()**, который на самом деле ничего не делает, а просто переводит модель в стандартный обучающий режим. Существует и ряд других режимов работы модели (например, верификация), поэтому данный флажок надо устанавливать явно.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HEobzwlg-MI-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'a': tensor([0.3367]), 'b': tensor([0.1288])})\n",
      "OrderedDict({'a': tensor([1.0235]), 'b': tensor([1.9690])})\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "# создаём модель\n",
    "model = ManualLinearRegression().to(device)\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train() # режим обучения\n",
    "\n",
    "    yhat = model(x_train_tensor) # \"запускаем\" модель с входными данными\n",
    "\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-w3zxZ8ZNrg"
   },
   "source": [
    "Получим такие результаты, фактически не отличающиеся от предыдущих версий программы:\n",
    "\n",
    "OrderedDict([('a', tensor([0.3367])), ('b', tensor([0.1288]))])\n",
    "\n",
    "OrderedDict([('a', tensor([1.0235])), ('b', tensor([1.9690]))])\n",
    "\n",
    "Теперь в основном коде мы вообще избавились от всех явных упоминаний параметров a и b! Чтобы изучить поведение любых других моделей, достаточно лишь сменить название класса ManualLinearRegression, причём это можно прозрачно автоматизировать.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOjhYe_QZYAs"
   },
   "source": [
    "##**Вложенные модели**\n",
    "\n",
    "PyTorch -- очень мощный по своим возможностям фреймворк. Он создавался, когда в машинном обучении уже был накоплен немалый опыт, и поэтому воплотил многие сильные идеи, которые в других ML-фреймворках либо не реализованы, либо реализованы плохо или неудобно.\n",
    "\n",
    "Казалось бы, мы ушли почти от всей ручной работы, и теперь работаем только с абстракцией модели, где задаём конкретную формулу прогноза и пару параметров, которые в этой формуле используются. Однако для большинства прогнозов можно воспользоваться уже готовыми моделями PyTorch.\n",
    "\n",
    "В частности, за линейную регрессию отвечает модель Linear\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html?source=post_page---------------------------#torch.nn.Linear\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "mEW3L_m6ViWq"
   },
   "outputs": [],
   "source": [
    "class LayerLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hI_0PO2XZsQ8"
   },
   "source": [
    "Вместо создания двух атрибутов линейной регрессии мы готовим в нашей модели всего один атрибут linear, который будет хранить вложенную модель Linear. В её конструкторе указываются два параметра -- количество наборов данных на входе и на выходе. И в том, и в другом случае их по одному: Linear(1, 1). А в методе forward() нашей модели мы просто \"запускаем\" вложенную модель Linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "rSGPGofRVyF8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'linear.weight': tensor([[0.7645]]), 'linear.bias': tensor([0.8300])})\n",
      "OrderedDict({'linear.weight': tensor([[1.9690]]), 'linear.bias': tensor([1.0235])})\n"
     ]
    }
   ],
   "source": [
    "from torch import optim, nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = LayerLinearRegression().to(device)\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    yhat = model(x_train_tensor)\n",
    "    loss = loss_fn(yhat, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "print(model.state_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcGCqmgFZ8dY"
   },
   "source": [
    "##**Последовательные модели**\n",
    "\n",
    "Пока мы использовали совсем простую модель в один шаг (слой) с помощью линейной регрессии, однако PyTorch активнее всего применяется для создания нейронных сетей, концепция которых довольно проста: есть наборы слоёв, каждый из которых это по сути функция над тензорами, и данные просто прогоняются через такую последовательность слоёв (выход одного слоя есть вход для следующего).\n",
    "\n",
    "Такая схема поддерживается в PyTorch последовательной моделью (тип nn.Sequential). В конструкторе она получает серию моделей, которые внутри автоматически связываются в последовательность вычислений через входы-выходы.\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html?source=post_page---------------------------#torch.nn.Sequential\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJmR8XGJaTlO"
   },
   "source": [
    "В нашем случае мы используем всего один слой Linear, и соответствующая последовательная модель может быть сформирована так:\n",
    "\n",
    "`model = nn.Sequential(nn.Linear(1, 1)).to(device)`\n",
    "\n",
    "Таким образом, мы вообще полностью избавились от всех пользовательских формул и типов данных, полностью сконструировав наш оптимизационный алгоритм из готовых кубиков PyTorch!\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xh9X8nXfu03e"
   },
   "source": [
    "##**Шаг обучения**\n",
    "\n",
    "Давайте взгляем на наш код ещё раз. Мы уже обобщили использование оптимизатора, функции потерь и модели. Эта схема универсальная, она по сути никак не изменится, если мы захотим использовать другой оптимизатор, или другой лосс, или другие модели. Но тогда может быть и эту схему можно ещё более обобщить?\n",
    "\n",
    "Напрашивается вариант, когда мы эти три сущности, а также метки и признаки, подаём в некоторый универсальный алгоритм просто как настроечные параметры. Такой универсальный алгоритм называется **шаг обучения**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "RfRVlTi5d10E"
   },
   "outputs": [],
   "source": [
    "# шаг обучения\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    # Формируем функцию, которая выполнит один шаг обучения\n",
    "    def train_step(x, y):\n",
    "        # Переводим модель в режим обучения\n",
    "        model.train()\n",
    "        # Вычислаем прогноз\n",
    "        yhat = model(x)\n",
    "        # Считаем лосс\n",
    "        loss = loss_fn(yhat, y)\n",
    "        # Вычисляем градиенты\n",
    "        loss.backward()\n",
    "        # Обновляем параметры и обнуляем градиенты\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Возвращаем лосс\n",
    "        return loss.item()\n",
    "\n",
    "    # Возвращаем функцию для вызова внутри цикла обучения\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iLSayQsUvC5c"
   },
   "source": [
    "Используем этот алгоритм для нашего конкретного случая:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "bw8O6D8XesBg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'0.weight': tensor([[0.7645]]), '0.bias': tensor([0.8300])})\n",
      "OrderedDict({'0.weight': tensor([[1.9690]]), '0.bias': tensor([1.0235])})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch import optim, nn\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(1, 1)).to(device)\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "# создадим функцию на основе модели, лосса и оптимизатора\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Выполним очередной шаг обучения ...\n",
    "    loss = train_step(x_train_tensor, y_train_tensor)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrsfvqAyvOcY"
   },
   "source": [
    "##**Датасеты (Datasets)**\n",
    "\n",
    "Продолжим оптимизацию и генерализацию нашей и так уже весьма универсальной программы. Сейчас мы преобразовываем массивы NumPy в тензоры PyTorch, но возможно есть более общий подход к представлению данных?\n",
    "\n",
    "В PyTorch работу с данными принято вести с помощью **датасетов (Dataset, набор данных)**, которые можно трактовать как **питоновские списки кортежей, где каждый кортеж задаёт одну точку** (признак, метку, ...).\n",
    "\n",
    "В конструкторе класса Dataset мы можем задавать самые разные формы представления входных данных (списка кортежей), от двух тензоров до CSV-файлов, и т. п.\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html?source=post_page---------------------------#torch.utils.data.Dataset\n",
    "\n",
    "Совсем не обязательно загружать в датасет сразу все данные -- ведь это могут быть обучающие выборки из десятков тысяч изображений. В таком случае их лучше загружать по прямому требованию программы. Для этого предназначен метод __get_item__(), который индексирует входной набор, позволяя обращаться к его элементам по индексам как к обычному массиву (по индексу возвращается соответствующий кортеж).\n",
    "\n",
    "Метод __len__() возвращает количество элементов во всём датасете.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Подготовим класс для наших данных.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Xb7TyCECiZSI"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x_tensor, y_tensor):\n",
    "        super().__init__()\n",
    "        self.x = x_tensor\n",
    "        self.y = y_tensor\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (self.x[index], self.y[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKouQy9nJhmc"
   },
   "source": [
    "Исходные данные преобразуем из формата массивов NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "h9gwgpNNjZI-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n"
     ]
    }
   ],
   "source": [
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = CustomDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZUUtDDOJ4EW"
   },
   "source": [
    "Но зачем вообще нужен дополнительный класс, если мы просто используем два тензора?\n",
    "\n",
    "Действительно, если наш датасет -- всего два тензора, то можно задействовать готовый класс PyTorch, который называется TensorDataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "EDz1E4cxj-EM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([0.7713]), tensor([2.4745]))\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "x_train_tensor = torch.from_numpy(x_train).float()\n",
    "y_train_tensor = torch.from_numpy(y_train).float()\n",
    "\n",
    "train_data = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL3kK_y8KF8K"
   },
   "source": [
    "Обратите внимание, что сейчас мы не загружаем датасеты в GPU (не отправляем их на девайс), и по умолчанию они работают на обычном процессоре. Если обучающая выборка большая, то хранить её лучше в оперативной памяти компьютера, а не графической платы.\n",
    "\n",
    "Но зачем всё же мы специально создаём датасеты? Потому что вместе с ними очень удобно использовать мощные загрузчики данных.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##**Загрузчик данных (DataLoader)**\n",
    "\n",
    "Пока в нашем простеньком примере для пакетного градиентного спуска использовались все данные целиком. В реальных проектах так бывает редко -- обучающие выборки могут быть очень велики. Поэтому наш датасет надо научиться делить на более мелкие наборы, **мини-пакеты**.\n",
    "\n",
    "В PyTorch для этого имеется специальный класс DataLoader\n",
    "\n",
    "https://pytorch.org/docs/stable/data.html?source=post_page---------------------------#torch.utils.data.DataLoader\n",
    "\n",
    "Идея его проста: мы задаём, какой набор данных использовать, какой желателен размер мини-пакета, и требуется ли данные этого мини-пакета перемешивать на каждой эпохе.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ysrHYYIY5uHw"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=16, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YImslgAZKrXy"
   },
   "source": [
    "Загрузчик работает как итератор: мы просто запрашиваем у него очередную порцию данных.\n",
    "\n",
    "Получать в цикле два очередных тензора с подвыборкой можно например так:\n",
    "\n",
    "`for x_batch, y_batch in train_loader:`\n",
    "\n",
    "Тогда новая версия нашей программы будет такая:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "H05Tdwgg6Kme"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'0.weight': tensor([[1.9696]]), '0.bias': tensor([1.0243])})\n"
     ]
    }
   ],
   "source": [
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P3r_ebYcK-Si"
   },
   "source": [
    "Обратите внимание, что теперь мы явно посылаем наши мини-пакеты на девайс, где развёрнута модель, потому что, как говорилось выше, датасеты исходно создаются в оперативной памяти. В частности, когда используются большие фермы с множеством графических плат, можно распределять эти мини-пакеты по разным GPU.\n",
    "\n",
    "И второе изменение -- добавился внутренний цикл, в котором мы последовательно получаем и обрабатываем кусочки исходной обучающей выборки.\n",
    "\n",
    "Итак, теперь по сути мы можем полностью сфокусироваться на обучающей выборке. Мы создаём датасет и загрузчик данных для него. То же самое мы делаем и для тестовой выборки, которую мы формировали вручную.\n",
    "\n",
    "Но нельзя ли автоматизировать и этот момент?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "##**Случайное разделение выборки**\n",
    "\n",
    "Именно для цели разделения выборки на обучающий и тестовый наборы в PyTorch имеется метод random_split(). Только, конечно, не надо забывать, что он исходно применяется ко всему входному массиву!\n",
    "\n",
    "random_split() получает на вход исходный датасет и список из двух чисел, первое из которых задаёт (в процентах) размер обучающей выборки, а второе число -- размер тестовой выборки.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XblNWaq4CAgC"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "x_tensor = torch.from_numpy(x).float()\n",
    "y_tensor = torch.from_numpy(y).float()\n",
    "\n",
    "dataset = TensorDataset(x_tensor, y_tensor)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [80, 20])\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=16)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwa-_aBWLa6O"
   },
   "source": [
    "##**Оценка**\n",
    "\n",
    "Заключительная часть посвящена оценке успешности нашей модели,\n",
    "что можно сделать через расчёт лосса для тестовой выборки. Для этого нам надо добавить дополнительный цикл итераций по мини-пакетам уже не обучающей, а тестовой выборки, и отправить их на тот же девайс, где и модель.\n",
    "\n",
    "Тут надо учесть два момента: во-первых, нам для этого уже не нужно считать градиенты, потому что выборка не обучающая, а тестовая (применяем упомянутый выше torch.no_grad() ко всему такому циклу), и во-вторых, модель надо явно перевести из обучающего режима в оценочный/тестовый с помощью метода eval().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "EL_pKl5deHwy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'0.weight': tensor([[1.9586]]), '0.bias': tensor([1.0195])})\n",
      "0.004001074470579624 tensor(0.0165)\n"
     ]
    }
   ],
   "source": [
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # обучающая выборка\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "\n",
    "    # тестовая выборка\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            model.eval()\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(yhat, y_val)\n",
    "\n",
    "print(model.state_dict())\n",
    "print(loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geXpBERlMay4"
   },
   "source": [
    "##**Задание**\n",
    "\n",
    "Оформите ваш код из последнего задания первого занятия так, чтобы он представлял собой работу с моделью.\n",
    "Добавьте верификацию модели по тестовой выборке, как в последнем примере.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "class PolynomialRegression(nn.Module):\n",
    "\n",
    "    def __init__(self, order: int):\n",
    "        super().__init__()\n",
    "        self._order = order\n",
    "        for i in range(self._order):\n",
    "            self.__setattr__(f\"p{i}\", nn.Parameter(torch.randn(1, requires_grad=True, dtype=torch.float)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return sum([self.__getattr__(f\"p{i}\") * (x ** i) for i in range(self._order)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'p0': tensor([0.3367]), 'p1': tensor([0.1288])})\n",
      "OrderedDict({'p0': tensor([1.0195]), 'p1': tensor([1.9586])})\n",
      "0.004001081455498934 tensor(0.0165)\n"
     ]
    }
   ],
   "source": [
    "model = PolynomialRegression(order=2).to(device)\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # обучающая выборка\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "\n",
    "    # тестовая выборка\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            model.eval()\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(yhat, y_val)\n",
    "\n",
    "print(model.state_dict())\n",
    "print(loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'p0': tensor([0.3820]), 'p1': tensor([0.0225]), 'p2': tensor([0.7218])})\n",
      "OrderedDict({'p0': tensor([1.0538]), 'p1': tensor([1.7296]), 'p2': tensor([0.2347])})\n",
      "0.0039096493273973465 tensor(0.0155)\n"
     ]
    }
   ],
   "source": [
    "model = PolynomialRegression(order=3).to(device)\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # обучающая выборка\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "\n",
    "    # тестовая выборка\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            model.eval()\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(yhat, y_val)\n",
    "\n",
    "print(model.state_dict())\n",
    "print(loss, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'p0': tensor([-0.2194]), 'p1': tensor([-1.1979]), 'p2': tensor([0.1434]), 'p3': tensor([0.4092]), 'p4': tensor([0.0514])})\n",
      "OrderedDict({'p0': tensor([1.0844]), 'p1': tensor([1.3744]), 'p2': tensor([0.8629]), 'p3': tensor([0.2061]), 'p4': tensor([-0.5752])})\n",
      "0.0030768527649343014 tensor(0.0152)\n"
     ]
    }
   ],
   "source": [
    "model = PolynomialRegression(order=5).to(device)\n",
    "print(model.state_dict())\n",
    "\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # обучающая выборка\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "\n",
    "    # тестовая выборка\n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in val_loader:\n",
    "            x_val = x_val.to(device)\n",
    "            y_val = y_val.to(device)\n",
    "            model.eval()\n",
    "            yhat = model(x_val)\n",
    "            val_loss = loss_fn(yhat, y_val)\n",
    "\n",
    "print(model.state_dict())\n",
    "print(loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6pFZz4vDMKcU"
   },
   "source": [
    "##**Итог**\n",
    "\n",
    "В первых двух занятиях мы изучили почти все ключевые шаги, которые делаются в практических проектах на PyTorch при разработке и анализе моделей машинного обучения!\n",
    "\n",
    "Далее мы будем рассматривать конкретные прикладные примеры и приёмы использования PyTorch с помощью нашей универсальной схемы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "sYVVNj5fqIa2",
    "BOjhYe_QZYAs",
    "XcGCqmgFZ8dY",
    "Xh9X8nXfu03e",
    "VrsfvqAyvOcY",
    "hL3kK_y8KF8K",
    "P3r_ebYcK-Si",
    "wwa-_aBWLa6O"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
