{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHcbu3iIq6HI"
   },
   "source": [
    "# **Занятие 4. Магия нейронных сетей**\n",
    "\n",
    "https://vk.com/lambda_brain\n",
    "\n",
    "Мы переходим к созданию полноценной нейронной сети, с помощью которой проанализируем весьма запутанный датасет с тремя классами.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xY30jKm16_k"
   },
   "source": [
    "## **Визуализация сложных моделей**\n",
    "\n",
    "Воспользуемся примером\n",
    "\n",
    "https://github.com/Atcold/pytorch-Deep-Learning-Minicourse/blob/master/04-spiral_classification.ipynb\n",
    "\n",
    "где генерируется подходящий набор данных из трёх классов, и применим к нему наш универсальный шаблон.\n",
    "\n",
    "Сперва импортируем нужные пакеты.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cy0ozQsoq6HK"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import numpy as np\n",
    "import math\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_gObksA2FeD"
   },
   "source": [
    "В оригинальном примере используется маленькая, но очень-очень классная библиотека plot_lib, упрощающая рисование графиков на основе модели. В конце занятия вы наглядно увидите её мощь. Чтобы не возиться с её установкой, сразу включим в наш проект."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tto41N_iq6HO"
   },
   "outputs": [],
   "source": [
    "# https://github.com/Atcold/pytorch-Deep-Learning-Minicourse/blob/master/plot_lib.py\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_default(figsize=(10, 10)):\n",
    "    plt.style.use(['dark_background', 'bmh'])\n",
    "    plt.rc('axes', facecolor='k')\n",
    "    plt.rc('figure', facecolor='k')\n",
    "    plt.rc('figure', figsize=figsize)\n",
    "\n",
    "\n",
    "def plot_data(X, y, d=0, auto=False, zoom=1):\n",
    "    X = X.cpu()\n",
    "    y = y.cpu()\n",
    "    plt.scatter(X.numpy()[:, 0], X.numpy()[:, 1], c=y, s=20, cmap=plt.cm.Spectral)\n",
    "    plt.axis('square')\n",
    "    plt.axis(np.array((-1.1, 1.1, -1.1, 1.1)) * zoom)\n",
    "    if auto is True: plt.axis('equal')\n",
    "    plt.axis('off')\n",
    "\n",
    "    _m, _c = 0, '.15'\n",
    "    plt.axvline(0, ymin=_m, color=_c, lw=1, zorder=0)\n",
    "    plt.axhline(0, xmin=_m, color=_c, lw=1, zorder=0)\n",
    "\n",
    "\n",
    "def plot_model(X, y, model):\n",
    "    model.cpu()\n",
    "    mesh = np.arange(-1.1, 1.1, 0.01)\n",
    "    xx, yy = np.meshgrid(mesh, mesh)\n",
    "    with torch.no_grad():\n",
    "        data = torch.from_numpy(np.vstack((xx.reshape(-1), yy.reshape(-1))).T).float()\n",
    "        Z = model(data).detach()\n",
    "    Z = np.argmax(Z, axis=1).reshape(xx.shape)\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.3)\n",
    "    plot_data(X, y)\n",
    "\n",
    "\n",
    "def show_scatterplot(X, colors, title=''):\n",
    "    colors = colors.cpu().numpy()\n",
    "    X = X.cpu().numpy()\n",
    "    plt.figure()\n",
    "    plt.axis('equal')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=colors, s=30)\n",
    "    # plt.grid(True)\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "def plot_bases(bases, width=0.04):\n",
    "    bases = bases.cpu()\n",
    "    bases[2:] -= bases[:2]\n",
    "    plt.arrow(*bases[0], *bases[2], width=width, color=(1,0,0), zorder=10, alpha=1., length_includes_head=True)\n",
    "    plt.arrow(*bases[1], *bases[3], width=width, color=(0,1,0), zorder=10, alpha=1., length_includes_head=True)\n",
    "\n",
    "\n",
    "def show_mat(mat, vect, prod, threshold=-1):\n",
    "    # Subplot grid definition\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, sharex=False, sharey=True,\n",
    "                                        gridspec_kw={'width_ratios':[5,1,1]})\n",
    "    # Plot matrices\n",
    "    cax1 = ax1.matshow(mat.numpy(), clim=(-1, 1))\n",
    "    ax2.matshow(vect.numpy(), clim=(-1, 1))\n",
    "    cax3 = ax3.matshow(prod.numpy(), clim=(threshold, 1))\n",
    "\n",
    "    # Set titles\n",
    "    ax1.set_title(f'A: {mat.size(0)} \\u00D7 {mat.size(1)}')\n",
    "    ax2.set_title(f'a^(i): {vect.numel()}')\n",
    "    ax3.set_title(f'p: {prod.numel()}')\n",
    "\n",
    "    # Plot colourbars\n",
    "    fig.colorbar(cax1, ax=ax2)\n",
    "    fig.colorbar(cax3, ax=ax3)\n",
    "\n",
    "    # Fix y-axis limits\n",
    "    ax1.set_ylim(bottom=max(len(prod), len(vect)) - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXZHR7Nm2Zaz"
   },
   "source": [
    "Выполним начальную инициализацию графики (set_default() из plot_lib)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qisy7RBsq6HS"
   },
   "outputs": [],
   "source": [
    "set_default()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcCcSpt92gN3"
   },
   "source": [
    "Изучите в ноутбуке вводную теорию по многослойной нейронной сети (пункт 4):\n",
    "\n",
    "https://github.com/DLSchool/dlschool/tree/master/06.%20PyTorch\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D0VaUh2s2n7Z"
   },
   "source": [
    "## **Однослойная нейронная сеть**\n",
    "\n",
    "Далее мы сперва создадим нейронную сеть только из двух слоёв -- входного и выходного. Это конечно на самом деле никакая не нейронная сеть, а по сути просто линейная модель, так как в ней пока не используются нейроны промежуточного слоя, где и происходит вся магия (способы эффективного обучения скрытых слоёв были придуманы относительно недавно). Хотя формально она называется **однослойная нейронная сеть**, потому что входной слой слоем не считается, а вот выходной считается.\n",
    "\n",
    "Затем мы добавим линейной модели скрытый слой из ста нейронов и сравним эффективности.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Зададим сперва так называемые **гипер-параметры** -- параметры, которые не меняются в процессе обучения нейросети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q4r95Hfuq6Ha"
   },
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "N = 1000  # количество точек на признак\n",
    "D = 2  # размерность данных (плоскость)\n",
    "C = 3  # количество признаков\n",
    "H = 100  # количество нейронов в скрытом слое\n",
    "\n",
    "# инициализируем девайс\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zXRi2zwO3F5g"
   },
   "source": [
    "В предыдущем примере и линейная модель, и модель с одним нейроном, показывали неплохие результаты. Проверим их на более сложных данных.\n",
    "\n",
    "Воспользуемся оригинальной схемой генерации датасета.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IKbgJayJq6Hc"
   },
   "outputs": [],
   "source": [
    "# пустые тензоры, хитро заполняющиеся данными\n",
    "X = torch.zeros(N * C, D).to(device) # признаки\n",
    "y = torch.zeros(N * C, dtype=torch.long).to(device) # прогноз: 0, 1 или 2\n",
    "\n",
    "for c in range(C):\n",
    "    index = 0\n",
    "    t = torch.linspace(0, 1, N)\n",
    "    inner_var = torch.linspace(\n",
    "        (2 * math.pi / C) * (c),\n",
    "        (2 * math.pi / C) * (2 + c),\n",
    "        N\n",
    "    ) + torch.randn(N) * 0\n",
    "\n",
    "    for ix in range(N * c, N * (c + 1)):\n",
    "        X[ix] = t[index] * torch.FloatTensor((\n",
    "            math.sin(inner_var[index]), math.cos(inner_var[index])\n",
    "        ))\n",
    "        y[ix] = c\n",
    "        index += 1\n",
    "\n",
    "print(\"X:\", X.shape)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fo7Ge17wq6Hf"
   },
   "outputs": [],
   "source": [
    "# визуализируем данные\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nh-QQtmx3Q_p"
   },
   "source": [
    "На картинке видны три класса значений -- три закручивающиеся кривые разных цветов. Теперь воспользуемся уже проверенной схемой для создания модели, выдающей прогнозы по такому набору данных.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Наша линейная модель запишется так:\n",
    "\n",
    "\n",
    "```\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H, C))\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkUqDRHZq6Hi"
   },
   "source": [
    "Мы подготовили промежуточную связь из 100 \"коннекторов\", которые пока просто соединяются друг с другом (100 выходов первой линейной модели соединяются с 100 входами второй линейной модели), как и положено однослойной нейронной сети.\n",
    "\n",
    "Такие сети, где информация обрабатывается последовательно, от входов к выходам, называются **сети прямого распространения (feedforward neural networks, FNN / FF)** .\n",
    "\n",
    "На следующем шаге мы добавим скрытый слой, а пока в качестве функции потерь укажем CrossEntropyLoss(). Это так называемая кросс- (перекрёстная) энтропия, которая отличается от квадратичной функции ошибок тем, что учитывает наличие у ошибки гауссового распределения.\n",
    "\n",
    "Как обычно, корректировка нашего шаблона минимальна: указываем только модель и функцию потерь.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmffGrixprME"
   },
   "outputs": [],
   "source": [
    "# импортируем нужные библиотеки\n",
    "import torch\n",
    "import numpy as np # всегда пригодится :)\n",
    "from torch.nn import Linear, Sigmoid\n",
    "\n",
    "# инициализируем девайс\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# добавляем типовую функцию \"шаг обучения\"\n",
    "def make_train_step(model, loss_fn, optimizer):\n",
    "    def train_step(x, y):\n",
    "        model.train()\n",
    "        yhat = model(x)\n",
    "        loss = loss_fn(yhat, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        return loss.item()\n",
    "    return train_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e8CyB-20pzhd"
   },
   "outputs": [],
   "source": [
    "from torch import optim, nn\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.Linear(H, C))\n",
    "model.to(device)\n",
    "\n",
    "lambda_l2 = 0.00001\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, weight_decay=lambda_l2)\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "   loss = train_step(X, y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wQ5uHaKjq6Hp"
   },
   "outputs": [],
   "source": [
    "# рисуем обученную модель и визуально показываем,\n",
    "# как распределились классы\n",
    "print(model)\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmT017ZT4Bg6"
   },
   "source": [
    "Обратите внимание, как красиво и как наглядно работает функция plot_model() -- она выводит как сам датасет, так и соответствующие классам области в обученной модели.\n",
    "\n",
    "Так как мы используем линейную модель, то и деление области датасета на три части выполняется только с помощью прямых, поэтому конечно и точность получается весьма условной. Хотя конечно для быстрой и грубой оценки даже такая модель будет работать удовлетворительно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U1y_i4FS4PP4"
   },
   "source": [
    "## **Двухслойная нейронная сеть**\n",
    "\n",
    "Теперь внесём в наш алгоритм две маленькие поправки.\n",
    "\n",
    "Во-первых, добавим наконец нашей модели скрытый промежуточный слой.\n",
    "\n",
    "Какую функцию активации использовать? Одно из стандартных решений -- это **ReLU**, довольно странная на первый взгляд функция, которая просто возвращает x, если x больше ноля, или ноль в противном случае. Но как ни удивительно, именно ReLU подходит к многим типовым задачам, причём комбинируя функции ReLU, можно аппроксимировать любую другую функцию, а сама она, очевидно, нетребовательна к ресурсам в плане реализации. Поэтому ReLU рекомендуется к применению в качестве первого шага эксперимента.\n",
    "\n",
    "Про функции активации и их выбор:\n",
    "\n",
    "https://neurohive.io/ru/osnovy-data-science/activation-functions/\n",
    "\n",
    "И вторая поправка -- заменим алгоритм оптимизации SGD на более точный в данном случае Adam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARumt2ZmCMSD"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(D, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, C)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "lambda_l2 = 0.00001\n",
    "lr = 0.1\n",
    "n_epochs = 1000\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=lambda_l2)\n",
    "train_step = make_train_step(model, loss_fn, optimizer)\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "   loss = train_step(X, y)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ioIBBHci5DfW"
   },
   "source": [
    "Посмотрите, как теперь красиво и точно обучилась наша модель!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBKfy93rq6H1"
   },
   "outputs": [],
   "source": [
    "# визуализируем\n",
    "print(model)\n",
    "plot_model(X, y, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nNcJIAQY5Nej"
   },
   "source": [
    "## **Задание**\n",
    "\n",
    "Найдите самостоятельно подобные достаточно сложные датасеты (или код их генерации) из нескольких (четыре и больше) классов значений, хорошо обучите двухслойную нейронную сеть и визуализируйте результат с помощью plot_lib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zsej5ZH5mvF"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "В следующем занятии мы переходим к популярной теме классификации изображений -- рукописных цифр."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2xY30jKm16_k",
    "D0VaUh2s2n7Z",
    "U1y_i4FS4PP4"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
