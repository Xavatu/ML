{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ov4K97EJbu4"
      },
      "source": [
        "# **Занятие 6. Свёрточные нейронные сети. Классификация цветных изображений**\n",
        "\n",
        "https://vk.com/lambda_brain\n",
        "\n",
        "Рассмотрим случай, когда входные данные -- это цветные изображения. Для обработки таких данных были придуманы **свёрточные нейронные сети**, воспользуемся для этого одной из классических моделей.\n",
        "\n",
        "---\n",
        "\n",
        "Импортируем нужные пакеты:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8abpHoRq6V-L"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# инициализируем девайс\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goNhmuPOJ-BV"
      },
      "source": [
        "Будем обрабатывать стандартный датасет **CIFAR10**, который включает фотографии десяти классов: самолёт, автомобиль, птица, кот, олень, собака, лягушка, лошадь, корабль и грузовик. На каждый класс приходится по 6000 (5000 обучающих и 1000 тестовых) цветных изображений размером 32 * 32 пиксела (и три канала цветности RGB)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UfwVbruB8gQb"
      },
      "outputs": [],
      "source": [
        "input_size = 3*32*32   # Размер изображения в точках * количество цветов\n",
        "num_classes = 10       # Количество распознающихся классов (10 видов изображений)\n",
        "n_epochs = 2           # Количество эпох\n",
        "batch_size = 4         # Размер мини-пакета входных данных\n",
        "lr = 0.001             # Скорость обучения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPSaBAmrKJ5p"
      },
      "source": [
        "Из-за специфики представления цветных изображений мы не можем сразу брать их в сыром виде: сперва требуется нормализовать картинки, превратить их в изображения с интенсивностью цвета в диапазоне 0..1. Это делает стандартная функция **Normalize()** из torchvision.transforms, которой в качестве параметров в подобных случаях обычно указываются стандартные значения (среднее и стандартное отклонение) для такой нормализации (0.5, 0.5, 0.5).\n",
        "\n",
        "То есть нам нужно выполнить композицию трансформаций: сперва выполнить преобразование в тензоры, и затем нормализовать.\n",
        "\n",
        "Композицию выполняет стандартная функция torchvision.transforms **.Compose()**. Её результат мы и задаём в качестве параметра transform конструктора CIFAR10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0eX2jdtCLzWd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a741487-0bc5-4fce-c0e9-37b98c5e3b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 30963842.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "cifar_trainset = dsets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbncbyBiKid7"
      },
      "source": [
        "Аналогично подготовим и тестовый датасет:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "8jKhtNW7L7YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "048b36fa-c0fa-4e90-e65a-8fe98767b95c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "50000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "cifar_testset = dsets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "print(len(cifar_trainset))\n",
        "print(len(cifar_testset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lug1CvTJKmHb"
      },
      "source": [
        "Загрузим наши данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "brvDSwBVNqqS"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=cifar_trainset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=cifar_testset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MeC-wQZ0KpJq"
      },
      "source": [
        "Добавим стандартный шаг обучения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TCBltxfGOChj"
      },
      "outputs": [],
      "source": [
        "# импортируем нужные библиотеки\n",
        "import torch\n",
        "import numpy as np # всегда пригодится :)\n",
        "from torch.nn import Linear, Sigmoid\n",
        "\n",
        "# добавляем типовую функцию \"шаг обучения\"\n",
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    def train_step(x, y):\n",
        "        model.train()\n",
        "        yhat = model(x)\n",
        "        loss = loss_fn(yhat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        return loss.item()\n",
        "    return train_step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdK0PPStSWwH"
      },
      "source": [
        "Какую задействовать модель? Вообще, конструирование наиболее эффективных моделей -- это искусство плюс математика. На практике часто можно пользоваться либо простыми стандартными решениями (как было например в случае распознавания рукописных цифр), либо сложными моделями, которые под соответствующие классы задач придумали ведущие специалисты в ML.\n",
        "В нашем случае мы применим модель LeNet, предложенную Яном ЛеКуном -- она относится к так называемым **свёрточным нейронным сетям (Convolutional Neural Network, CNN)**, хорошо работающим с двумерными изображениями.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Подробное описание принципов работы CNN:\n",
        "\n",
        "https://neurohive.io/ru/tutorial/cnn-na-pytorch/\n",
        "\n",
        "Главное, обратите внимание на принцип **свёртки** и движущееся окно/**фильтр**.\n",
        "\n",
        "**Пулинг (pooling)** -- это схожая со скользящим окном техника, когда вместо свёртки по обучаемым весам к значениям в окне применяется некоторая статистическая функция (среднее, максимум, ...). Так, популярная механика тут -- это max pooling. Пулинг выполняет обобщение мелких деталей, устойчиво выделяет некоторый признак независимо от его размера и ориентации.\n",
        "\n",
        "**Канал** -- это множество фильтров, формирующих оригинальный двумерный вывод. Каналы нередко связываются с цветностью изображения.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XVIF0OHjvkRW"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CifarModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CifarModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoZi7iOLTDUH"
      },
      "source": [
        "Рассмотрим структуру этой модели подробнее.\n",
        "\n",
        "По мелким техническим причинам (необходимость выравнивания данных при переходе от свёрточных слоёв к линейным) её удобнее представить в формате классической модели, а не просто композицией слоёв.\n",
        "\n",
        "Первый слой -- Conv2d(3, 6, 5) с функцией активации ReLU создаёт набор свёрточных фильтров. Первый параметр 3 -- это количество входных каналов изображений, три цвета. Второй параметр 6 -- это количество выходных каналов, третий параметр -- размер фильтра 5x5. На выходе получается 6 фильтров размером 3x5x5 -- и всего модель выделяет (3 * 5 * 5 + 1) * 6 = 456 параметров.\n",
        "Выходной размер слоя получится 6 * 28 * 28 , где 28 = ((32 - 5) + 1)\n",
        "\n",
        "Метод MaxPool2d(2,2) -- это реализация max-пулинга (вычисление его аргументов см. по ссылке выше). kernel_size -- размер окна пулинга, stride -- шаг пулинга. Выходной размер слоя таким образом снижаем в два раза: с 6 * 28 * 28 до 6 * 14 * 14.\n",
        "\n",
        "\n",
        "Далее снова применяется функция Conv2d(6, 16, 5) -- шесть выходных каналов предыдущей функций как входы. Теперь мы применяем 16 фильтров (каждый размером 6 * 5 * 5), и выходной размер слоя будет 16 * 10 * 10, где 10 = (14 - 5) + 1. Всего на уровне обрабатывается (5 * 5 * 6 + 1) * 16 = 2416 параметров.\n",
        "\n",
        "Следующий max pooling снижает этот выход в два раза -- с 16 * 10 * 10 до 16 * 5 * 5.\n",
        "\n",
        "И в заключение добавляются три полносвязных слоя Linear. Обратите внимание, что перед ними надо выполнить модификацию структуры передаваемых данных, так как свёрточные слои работают с двумерными изображениями, а линейные -- с векторными наборами. Такое преобразование выполняет x.view(-1, 16 * 5 * 5).\n",
        "\n",
        "Первый линейный слой из 120 узлов, получает 16 * 5 * 5 входов -- то есть в нём требуется (16 * 5 * 5 + 1) * 120 = 48120 параметров, и далее количество входов-выходов понижается через следующие слои до наших итоговых 10 классов (последний уровень требует (84+1) * 10 = 850 параметров).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ZlW-CkjeOn2u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57988801-676c-4937-fb6a-5dabc98e69cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0 loss=1.0366183519363403\n",
            "epoch=1 loss=1.3498492240905762\n"
          ]
        }
      ],
      "source": [
        "from torch import optim, nn\n",
        "\n",
        "model = CifarModel()\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        loss = train_step(images, labels)\n",
        "    print(f\"{epoch=} {loss=}\")\n",
        "\n",
        "# print(model.state_dict())\n",
        "# print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "L-zC19n5X_-7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45fb2ed5-d85d-47d6-ab9f-23af518c4da1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность: 53.51 %\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # проверяем на тестовой выборке\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Точность: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56Kd4ku8b-9b"
      },
      "source": [
        "Считаем точность -- она получается в районе 55%. Это хороший результат, так как при случайном выборе мы получили бы 10%. Причём её можно существенно повысить -- уже после 10 эпох обучения мы получим точность 80+%!\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LX96AyHbcxSr"
      },
      "source": [
        "В заключение вычислим точность распознавания по каждому из признаков:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5LzRkaZXxb9k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2276220-6659-4cb2-f97d-bc84b158d3c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность для tensor(3, device='cuda:0') : 62 %\n",
            "Точность для tensor(5, device='cuda:0') : 82 %\n",
            "Точность для tensor(1, device='cuda:0') : 34 %\n",
            "Точность для tensor(7, device='cuda:0') : 24 %\n"
          ]
        }
      ],
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(4):\n",
        "    print('Точность для %5s : %2d %%' % (\n",
        "        labels[i], 100 * class_correct[i] / class_total[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVNjHoPddEUb"
      },
      "source": [
        "## **Задание.**\n",
        "Увеличение количества эпох (шагов обучения) существенно повышает качество модели. Но есть и другие способы -- поэкспериментируйте например с добавлением новых свёрточных или линейных слоёв, размерами фильтров и их количеством. В процессе экспериментов избегайте переобучения -- когда модель показывает отличные результаты на обучающей выборке, но невысокие на тестовой.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "В следующем занятии мы займёмся экспериментами с уже обученными моделями."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "7RP9rwdwUgNt"
      },
      "outputs": [],
      "source": [
        "input_size = 3*32*32   # Размер изображения в точках * количество цветов\n",
        "num_classes = 10       # Количество распознающихся классов (10 видов изображений)\n",
        "n_epochs = 10          # Количество эпох\n",
        "batch_size = 4         # Размер мини-пакета входных данных\n",
        "lr = 0.001             # Скорость обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0uvcqI7UgNt",
        "outputId": "b7e4b3bc-92c9-4052-bc5f-f00823854f82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "50000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "cifar_trainset = dsets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "cifar_testset = dsets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "print(len(cifar_trainset))\n",
        "print(len(cifar_testset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "v2gWei0xUgNt"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=cifar_trainset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=cifar_testset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6pGR9gBUgNu",
        "outputId": "01880eca-59fb-43a3-aca1-a4a9cd6d77db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0 loss=1.4364426136016846\n",
            "epoch=1 loss=1.9631304740905762\n",
            "epoch=2 loss=1.7222373485565186\n",
            "epoch=3 loss=0.9289392828941345\n",
            "epoch=4 loss=2.3496437072753906\n",
            "epoch=5 loss=0.5360375046730042\n",
            "epoch=6 loss=1.0796667337417603\n",
            "epoch=7 loss=1.7447887659072876\n",
            "epoch=8 loss=1.0040431022644043\n",
            "epoch=9 loss=0.25247350335121155\n"
          ]
        }
      ],
      "source": [
        "from torch import optim, nn\n",
        "\n",
        "model = CifarModel()\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        loss = train_step(images, labels)\n",
        "    print(f\"{epoch=} {loss=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DnUWXth3UgNu",
        "outputId": "8c2e6d46-160c-473e-8bdb-6551e5d72efe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность: 60.58 %\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # проверяем на тестовой выборке\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Точность: {} %'.format(100 * correct / total))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80avMd7vUgNu",
        "outputId": "e15c890b-fa21-4036-9533-acdab2eb6c8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность для tensor(3, device='cuda:0') : 65 %\n",
            "Точность для tensor(5, device='cuda:0') : 70 %\n",
            "Точность для tensor(1, device='cuda:0') : 48 %\n",
            "Точность для tensor(7, device='cuda:0') : 34 %\n"
          ]
        }
      ],
      "source": [
        "class_correct = list(0. for i in range(10))\n",
        "class_total = list(0. for i in range(10))\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        c = (predicted == labels).squeeze()\n",
        "        for i in range(4):\n",
        "            label = labels[i]\n",
        "            class_correct[label] += c[i].item()\n",
        "            class_total[label] += 1\n",
        "\n",
        "\n",
        "for i in range(4):\n",
        "    print('Точность для %5s : %2d %%' % (\n",
        "        labels[i], 100 * class_correct[i] / class_total[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqS_DZvYUgNu"
      },
      "source": [
        "**ResNet**\n",
        "\n",
        "Поскольку не удалось добиться точности свыше 62%, попробуем использовать ResNet\n",
        "\n",
        "Имплементация на PyTorch + четкое объяснение принципов работы ResNet\n",
        "https://debuggercafe.com/implementing-resnet18-in-pytorch-from-scratch/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qJTsn2BUgNu"
      },
      "source": [
        "Я слегка переписал имплементацию на более правильную версию с точки зрения ООП (как я вижу, могу ошибаться)\n",
        "\n",
        "https://github.com/Xavatu/ML/blob/master/resnet.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IuGw5P0LUgNu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# инициализируем девайс\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        stride: int,\n",
        "        expansion: int,\n",
        "        downsample: nn.Module = None,\n",
        "    ) -> None:\n",
        "        super(Block, self).__init__()\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class BasicBlock(Block):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        stride: int,\n",
        "        expansion: int,\n",
        "        downsample: nn.Module = None,\n",
        "    ) -> None:\n",
        "        super(Block, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.expansion = expansion\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=out_channels,\n",
        "            out_channels=out_channels * self.expansion,\n",
        "            kernel_size=3,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        identity = self.downsample(x) if self.downsample is not None else x\n",
        "        out = self.conv1(x)\n",
        "        out = self.batch_norm1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.batch_norm2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class BottleNeckBlock(Block):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        stride: int,\n",
        "        expansion: int,\n",
        "        downsample: nn.Module = None,\n",
        "    ) -> None:\n",
        "        super(Block, self).__init__()\n",
        "        self.downsample = downsample\n",
        "        self.expansion = expansion\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv0 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.batch_norm0 = nn.BatchNorm2d(out_channels)\n",
        "        in_channels = out_channels\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=in_channels,\n",
        "            out_channels=out_channels,\n",
        "            kernel_size=3,\n",
        "            stride=stride,\n",
        "            padding=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.batch_norm1 = nn.BatchNorm2d(out_channels)\n",
        "        self.conv2 = nn.Conv2d(\n",
        "            in_channels=out_channels,\n",
        "            out_channels=out_channels * self.expansion,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.batch_norm2 = nn.BatchNorm2d(out_channels * self.expansion)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        identity = self.downsample(x) if self.downsample is not None else x\n",
        "        out = self.conv0(x)\n",
        "        out = self.batch_norm0(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv1(out)\n",
        "        out = self.batch_norm1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.batch_norm2(out)\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class _ResNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_channels: int,\n",
        "        block: type[Block],\n",
        "        num_classes: int,\n",
        "        blocks_count: tuple[int, int, int, int],\n",
        "        expansion: int,\n",
        "    ) -> None:\n",
        "        super(_ResNet, self).__init__()\n",
        "        self.blocks_count = blocks_count\n",
        "        self.expansion = expansion\n",
        "        self.in_channels = 64\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_channels=img_channels,\n",
        "            out_channels=self.in_channels,\n",
        "            kernel_size=7,\n",
        "            stride=2,\n",
        "            padding=3,\n",
        "            bias=False,\n",
        "        )\n",
        "        self.batch_norm = nn.BatchNorm2d(self.in_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.max_pool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self._make_layer(\n",
        "            block=block, out_channels=64, count=self.blocks_count[0], stride=1\n",
        "        )\n",
        "        self.layer2 = self._make_layer(\n",
        "            block=block, out_channels=128, count=self.blocks_count[1], stride=2\n",
        "        )\n",
        "        self.layer3 = self._make_layer(\n",
        "            block=block, out_channels=256, count=self.blocks_count[2], stride=2\n",
        "        )\n",
        "        self.layer4 = self._make_layer(\n",
        "            block=block, out_channels=512, count=self.blocks_count[3], stride=2\n",
        "        )\n",
        "        self.adaptive_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512 * self.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(\n",
        "        self,\n",
        "        block: type[Block],\n",
        "        out_channels: int,\n",
        "        count: int,\n",
        "        stride: int,\n",
        "    ) -> nn.Sequential:\n",
        "        downsample = None\n",
        "        if stride != 1 or self.in_channels != out_channels * self.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(\n",
        "                    in_channels=self.in_channels,\n",
        "                    out_channels=out_channels * self.expansion,\n",
        "                    kernel_size=1,\n",
        "                    stride=stride,\n",
        "                    bias=False,\n",
        "                ),\n",
        "                nn.BatchNorm2d(out_channels * self.expansion),\n",
        "            )\n",
        "        layers = [\n",
        "            (\n",
        "                block(\n",
        "                    self.in_channels,\n",
        "                    out_channels,\n",
        "                    stride,\n",
        "                    self.expansion,\n",
        "                    downsample,\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "        self.in_channels = out_channels * self.expansion\n",
        "        layers += [\n",
        "            block(\n",
        "                in_channels=self.in_channels,\n",
        "                out_channels=out_channels,\n",
        "                stride=1,\n",
        "                expansion=self.expansion,\n",
        "            )\n",
        "            for _ in range(count - 1)\n",
        "        ]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.conv1(x)\n",
        "        x = self.batch_norm(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.max_pool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.adaptive_avg_pool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResNet18(_ResNet):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_channels: int,\n",
        "        num_classes: int,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            img_channels=img_channels,\n",
        "            block=BasicBlock,\n",
        "            num_classes=num_classes,\n",
        "            blocks_count=(2, 2, 2, 2),\n",
        "            expansion=1,\n",
        "        )\n",
        "\n",
        "\n",
        "class ResNet34(_ResNet):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_channels: int,\n",
        "        num_classes: int,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            img_channels=img_channels,\n",
        "            block=BasicBlock,\n",
        "            num_classes=num_classes,\n",
        "            blocks_count=(3, 4, 6, 3),\n",
        "            expansion=1,\n",
        "        )\n",
        "\n",
        "\n",
        "class ResNet50(_ResNet):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_channels: int,\n",
        "        num_classes: int,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            img_channels=img_channels,\n",
        "            block=BottleNeckBlock,\n",
        "            num_classes=num_classes,\n",
        "            blocks_count=(3, 4, 6, 3),\n",
        "            expansion=4,\n",
        "        )\n",
        "\n",
        "\n",
        "class ResNet101(_ResNet):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_channels: int,\n",
        "        num_classes: int,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            img_channels=img_channels,\n",
        "            block=BottleNeckBlock,\n",
        "            num_classes=num_classes,\n",
        "            blocks_count=(3, 4, 23, 3),\n",
        "            expansion=4,\n",
        "        )\n",
        "\n",
        "\n",
        "class ResNet152(_ResNet):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_channels: int,\n",
        "        num_classes: int,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            img_channels=img_channels,\n",
        "            block=BottleNeckBlock,\n",
        "            num_classes=num_classes,\n",
        "            blocks_count=(3, 8, 36, 3),\n",
        "            expansion=4,\n",
        "        )"
      ],
      "metadata": {
        "id": "ZUdmzUrUU8_A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "9hga9onqUgNu"
      },
      "outputs": [],
      "source": [
        "# добавляем типовую функцию \"шаг обучения\"\n",
        "def make_train_step(model, loss_fn, optimizer):\n",
        "    def train_step(x, y):\n",
        "        model.train()\n",
        "        yhat = model(x)\n",
        "        loss = loss_fn(yhat, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        return yhat, loss.item()\n",
        "    return train_step\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "U0FA_9igUgNu"
      },
      "outputs": [],
      "source": [
        "input_size = 3*32*32   # Размер изображения в точках * количество цветов\n",
        "num_classes = 10       # Количество распознающихся классов (10 видов изображений)\n",
        "n_epochs = 100         # Количество эпох\n",
        "batch_size = 256       # Размер мини-пакета входных данных\n",
        "lr = 0.01              # Скорость обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gytMSoqUgNv",
        "outputId": "066734ac-32c9-45cf-9430-c31bbe50d45d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "50000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "import torchvision.transforms as transforms\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4, padding_mode='reflect'),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "cifar_trainset = dsets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "cifar_testset = dsets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "print(len(cifar_trainset))\n",
        "print(len(cifar_testset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "bvpgyw3qUgNv"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(dataset=cifar_trainset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True,\n",
        "                                           num_workers=2)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=cifar_testset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False,\n",
        "                                          num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TwfXEmm2UgNv",
        "outputId": "0d71ab0d-afc6-4de8-b20c-9971133a31d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0 loss=1.5492775440216064 accuracy=40.364\n",
            "epoch=1 loss=1.555121660232544 accuracy=51.856\n",
            "epoch=2 loss=0.9267351031303406 accuracy=56.69\n",
            "epoch=3 loss=1.047095775604248 accuracy=60.284000000000006\n",
            "epoch=4 loss=1.1124536991119385 accuracy=62.864\n",
            "epoch=5 loss=1.0063002109527588 accuracy=65.034\n",
            "epoch=6 loss=0.8574200868606567 accuracy=66.682\n",
            "epoch=7 loss=1.0344228744506836 accuracy=68.662\n",
            "epoch=8 loss=0.8414037823677063 accuracy=69.834\n",
            "epoch=9 loss=0.5436530113220215 accuracy=71.36200000000001\n",
            "epoch=10 loss=0.8747626543045044 accuracy=72.55\n",
            "epoch=11 loss=0.8477487564086914 accuracy=73.844\n",
            "epoch=12 loss=0.7753360271453857 accuracy=75.102\n",
            "epoch=13 loss=0.4889650344848633 accuracy=75.952\n",
            "epoch=14 loss=0.6370298862457275 accuracy=76.584\n",
            "epoch=15 loss=0.822266697883606 accuracy=77.91799999999999\n",
            "epoch=16 loss=0.39303797483444214 accuracy=78.634\n",
            "epoch=17 loss=0.6396672129631042 accuracy=79.644\n",
            "epoch=18 loss=0.43366527557373047 accuracy=80.458\n",
            "epoch=19 loss=0.6026569604873657 accuracy=81.03399999999999\n",
            "epoch=20 loss=0.6255608797073364 accuracy=81.892\n",
            "epoch=21 loss=0.5649861097335815 accuracy=82.42\n",
            "epoch=22 loss=0.5337597131729126 accuracy=83.07\n",
            "epoch=23 loss=0.5076579451560974 accuracy=83.854\n",
            "epoch=24 loss=0.5559707880020142 accuracy=84.672\n",
            "epoch=25 loss=0.46290549635887146 accuracy=85.084\n",
            "epoch=26 loss=0.4795939326286316 accuracy=85.466\n",
            "epoch=27 loss=0.4656093120574951 accuracy=86.37599999999999\n",
            "epoch=28 loss=0.433097779750824 accuracy=86.786\n",
            "epoch=29 loss=0.5424585342407227 accuracy=87.38\n",
            "epoch=30 loss=0.3378414511680603 accuracy=87.494\n",
            "epoch=31 loss=0.2593219578266144 accuracy=88.232\n",
            "epoch=32 loss=0.35178282856941223 accuracy=88.752\n",
            "epoch=33 loss=0.28447332978248596 accuracy=88.986\n",
            "epoch=34 loss=0.2496812641620636 accuracy=89.39\n",
            "epoch=35 loss=0.37925466895103455 accuracy=89.72\n",
            "epoch=37 loss=0.3327351212501526 accuracy=90.628\n",
            "epoch=38 loss=0.33820122480392456 accuracy=91.054\n",
            "epoch=39 loss=0.23091503977775574 accuracy=91.14999999999999\n",
            "epoch=40 loss=0.1947130411863327 accuracy=91.71000000000001\n",
            "epoch=41 loss=0.29073572158813477 accuracy=91.646\n",
            "epoch=42 loss=0.31727200746536255 accuracy=92.414\n",
            "epoch=43 loss=0.25064200162887573 accuracy=92.494\n",
            "epoch=44 loss=0.11337935924530029 accuracy=92.84599999999999\n",
            "epoch=45 loss=0.2626808285713196 accuracy=93.138\n",
            "epoch=46 loss=0.24659042060375214 accuracy=93.266\n",
            "epoch=47 loss=0.12050609290599823 accuracy=93.28800000000001\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-c549ca3edefe>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mtrain_running_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-4714d7863508>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    290\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    766\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from torch import optim, nn\n",
        "\n",
        "model = ResNet18(\n",
        "    img_channels=3,\n",
        "    num_classes=10,\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "train_step = make_train_step(model, loss_fn, optimizer)\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    train_running_correct = 0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs, loss = train_step(images, labels)\n",
        "        _, preds = torch.max(outputs.data, 1)\n",
        "        train_running_correct += (preds == labels).sum().item()\n",
        "    accuracy = 100. * (train_running_correct / len(train_loader.dataset))\n",
        "    print(f\"{epoch=} {loss=} {accuracy=}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSVsPzHtUgNv",
        "outputId": "41ab5a91-3a45-46e4-aeb2-fc1de59bc16f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Точность: 75.71 %\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad(): # проверяем на тестовой выборке\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Точность: {} %'.format(100 * correct / total))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}